{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn_evaluation import plot\n",
    "\n",
    "print(\"Imports complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the text file, ignoring the first line\n",
    "\n",
    "filename=\"./data/MiniBooNE_PID.txt\"\n",
    "data=pd.read_csv(filename,delim_whitespace=True,skiprows=1,header=None)\n",
    "print(data.head(1))\n",
    "X=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Create target vector for each row of the dataset. \n",
    "The first value in the first row contains the number of signal events, the second the number of background events. \n",
    "The target vector should contain 1 for signal events and 0 for background events.\"\"\"\n",
    "\n",
    "# Get number of signal and background events\n",
    "signal, background = open(filename).readline().split()\n",
    "\n",
    "y = np.concatenate((np.ones(int(signal)), np.zeros(int(background))))\n",
    "print(y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization scales the input features to have a mean of 0 and a standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the input data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" save the scaled matrix and the target vector as npy files \"\"\"\n",
    "np.save('X.npy', X_scaled)\n",
    "np.save('y.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.load('X.npy') # load the scaled matrix\n",
    "y=np.load('y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "# Create a vanilla neural network classifier and train it on the training data\n",
    "nn_clf = MLPClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter space to search over for the vanilla neural network classifier\n",
    "nn_param_grid = {\n",
    "    'hidden_layer_sizes': [(5,), (10,)],\n",
    "    'solver': ['adam'],\n",
    "    'activation': [\"relu\", \"logistic\", \"tanh\", \"identity\"],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Set up the grid search using 5-fold cross-validation\n",
    "# nn_grid_search = GridSearchCV(nn_clf, nn_param_grid, cv=5, scoring='accuracy')\n",
    "nn_grid_search = RandomizedSearchCV(nn_clf, nn_param_grid, cv=5, scoring='accuracy', n_iter=2)\n",
    "\n",
    "# Perform the grid search on the training set\n",
    "nn_start_time = time.time()\n",
    "nn_grid_search.fit(X_train, y_train)\n",
    "nn_end_time = time.time()\n",
    "print('Time required to grid search Vanilla Neural Network classifier:', nn_end_time - nn_start_time)\n",
    "\n",
    "# Print the best hyperparameters and the best cross-validation accuracy\n",
    "print('Vanilla Neural Network best hyperparameters:', nn_grid_search.best_params_)\n",
    "print('Vanilla Neural Network best cross-validation accuracy:', nn_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Re-train the classifier using the best hyperparameters on the full training set\n",
    "nn_clf = nn_grid_search.best_estimator_\n",
    "\n",
    "# nn_start_time = time.time()\n",
    "# nn_clf.fit(X_train, y_train)\n",
    "# nn_end_time = time.time()\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "# nn_test_accuracy = nn_clf.score(X_test, y_test)\n",
    "nn_test_accuracy = nn_grid_search.score(X_test, y_test)\n",
    "print('Time required to train Vanilla Neural Network classifier:', nn_end_time - nn_start_time)\n",
    "print('Vanilla Neural Network test accuracy:', nn_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\" Plot the scores for the searched parameters on the vanilla neural network classifier \"\"\"\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plot\u001b[39m.\u001b[39mgrid_search(nn_grid_search\u001b[39m.\u001b[39mcv_results_, change\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlearning_rate_init\u001b[39m\u001b[39m'\u001b[39m, kind\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\" Plot the scores for the searched parameters on the vanilla neural network classifier \"\"\"\n",
    "plot.grid_search(nn_grid_search.cv_results_, change='learning_rate_init', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVM classifier and train it on the training data\n",
    "svm_clf = SVC(random_state=42)\n",
    "\n",
    "svm_param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "svm_grid_search = GridSearchCV(svm_clf, svm_param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "svm_start_time = time.time()\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "svm_end_time = time.time()\n",
    "\n",
    "print('SVM best hyperparameters:', svm_grid_search.best_params_)\n",
    "print('SVM best cross-validation accuracy:', svm_grid_search.best_score_)\n",
    "\n",
    "svm_clf = svm_grid_search.best_estimator_\n",
    "\n",
    "# svm_clf.fit(X_train, y_train)\n",
    "\n",
    "svm_test_accuracy = svm_grid_search.score(X_test, y_test)\n",
    "print('Time required to train SVM classifier:', svm_end_time - svm_start_time)\n",
    "print('SVM test accuracy:', svm_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier and train it on the training data\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 50, 100, 200],\n",
    "    \"max_depth\": [None, 10, 20, 30],\n",
    "    \"min_samples_split\": [2, 4, 8],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(rf_clf, svm_param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "rf_start_time = time.time()\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "rf_end_time = time.time()\n",
    "\n",
    "print('RF best hyperparameters:', rf_grid_search.best_params_)\n",
    "print('RF best cross-validation accuracy:', rf_grid_search.best_score_)\n",
    "\n",
    "rf_clf = rf_grid_search.best_estimator_\n",
    "# rf_clf.fit(X_train, y_train)\n",
    "\n",
    "rf_test_accuracy = rf_grid_search.score(X_test, y_test)\n",
    "print('Time required to train RF classifier:', rf_end_time - rf_start_time)\n",
    "print('RF test accuracy:', rf_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the test set\n",
    "nn_predictions = nn_clf.predict(X_test)\n",
    "svm_predictions = svm_clf.predict(X_test)\n",
    "rf_predictions = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define a dictionary that maps model names to their hyperparameters\n",
    "model_hyperparams = {\n",
    "    'Vanilla Neural Network': nn_grid_search.best_params_,\n",
    "    'SVM': svm_grid_search.best_params_,\n",
    "    'Random Forest': rf_grid_search.best_params_\n",
    "}\n",
    "\n",
    "# Compute the confusion matrix for each model\n",
    "nn_confusion_matrix = metrics.confusion_matrix(y_test, nn_predictions)\n",
    "svm_confusion_matrix = metrics.confusion_matrix(y_test, svm_predictions)\n",
    "rf_confusion_matrix = metrics.confusion_matrix(y_test, rf_predictions)\n",
    "\n",
    "# Print the confusion matrix and hyperparameters for each model\n",
    "print('Vanilla Neural Network confusion matrix:')\n",
    "print(nn_confusion_matrix)\n",
    "print('Vanilla Neural Network hyperparameters:', model_hyperparams['Vanilla Neural Network'])\n",
    "\n",
    "print('SVM confusion matrix:')\n",
    "print(svm_confusion_matrix)\n",
    "print('SVM hyperparameters:', model_hyperparams['SVM'])\n",
    "\n",
    "print('Random Forest confusion matrix:')\n",
    "print(rf_confusion_matrix)\n",
    "print('Random Forest hyperparameters:', model_hyperparams['Random Forest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of each classifier\n",
    "nn_accuracy = metrics.accuracy_score(y_test, nn_predictions)\n",
    "svm_accuracy = metrics.accuracy_score(y_test, svm_predictions)\n",
    "rf_accuracy = metrics.accuracy_score(y_test, rf_predictions)\n",
    "print(\"Vanilla Neural Network Accuracy:\", nn_accuracy)\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80efd73a90444072c23059b27a682a4b0343647f1b8849b7f8cb3b27a325c7e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
