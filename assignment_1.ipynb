{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1         2         3         4         5         6   \\\n",
      "0  2.59413  0.468803   20.6916  0.322648  0.009682  0.374393  0.803479   \n",
      "1  3.86388  0.645781   18.1375  0.233529  0.030733  0.361239  1.069740   \n",
      "2  3.38584  1.197140   36.0807  0.200866  0.017341  0.260841  1.108950   \n",
      "3  4.28524  0.510155  674.2010  0.281923  0.009174  0.000000  0.998822   \n",
      "4  5.93662  0.832993   59.8796  0.232853  0.025066  0.233556  1.370040   \n",
      "\n",
      "         7        8         9   ...       40       41        42       43  \\\n",
      "0  0.896592  3.59665  0.249282  ...  101.174 -31.3730  0.442259  5.86453   \n",
      "1  0.878714  3.59243  0.200793  ...  186.516  45.9597 -0.478507  6.11126   \n",
      "2  0.884405  3.43159  0.177167  ...  129.931 -11.5608 -0.297008  8.27204   \n",
      "3  0.823390  3.16382  0.171678  ...  163.978 -18.4586  0.453886  2.48112   \n",
      "4  0.787424  3.66546  0.174862  ...  229.555  42.9600 -0.975752  2.66109   \n",
      "\n",
      "         44        45        46        47        48        49  \n",
      "0  0.000000  0.090519  0.176909  0.457585  0.071769  0.245996  \n",
      "1  0.001182  0.091800 -0.465572  0.935523  0.333613  0.230621  \n",
      "2  0.003854  0.141721 -0.210559  1.013450  0.255512  0.180901  \n",
      "3  0.000000  0.180938  0.407968  4.341270  0.473081  0.258990  \n",
      "4  0.000000  0.170836 -0.814403  4.679490  1.924990  0.253893  \n",
      "\n",
      "[5 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load the data from the text file, ignoring the first line\n",
    "\n",
    "filename=\"./data/MiniBooNE_PID.txt\"\n",
    "data=pd.read_csv(filename,delim_whitespace=True,skiprows=1,header=None)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
      "           dtype='int64')\n",
      "Index(['PID_1', 'PID_2', 'PID_3', 'PID_4', 'PID_5', 'PID_6', 'PID_7', 'PID_8',\n",
      "       'PID_9', 'PID_10', 'PID_11', 'PID_12', 'PID_13', 'PID_14', 'PID_15',\n",
      "       'PID_16', 'PID_17', 'PID_18', 'PID_19', 'PID_20', 'PID_21', 'PID_22',\n",
      "       'PID_23', 'PID_24', 'PID_25', 'PID_26', 'PID_27', 'PID_28', 'PID_29',\n",
      "       'PID_30', 'PID_31', 'PID_32', 'PID_33', 'PID_34', 'PID_35', 'PID_36',\n",
      "       'PID_37', 'PID_38', 'PID_39', 'PID_40', 'PID_41', 'PID_42', 'PID_43',\n",
      "       'PID_44', 'PID_45', 'PID_46', 'PID_47', 'PID_48', 'PID_49', 'PID_50'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# rename columns to PID_0 - PID_50\n",
    "print(data.keys())\n",
    "data.columns = [\"PID_\"+str(i) for i in range(1, data.shape[1]+1)]  # type: ignore\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(130064, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Create target vector for each row of the dataset. \n",
    "The first value in the first row contains the number of signal events, the second the number of background events. \n",
    "The target vector should contain 1 for signal events and 0 for background events.\"\"\"\n",
    "\n",
    "# Get number of signal and background events\n",
    "signal, background = open(filename).readline().split()\n",
    "\n",
    "y = np.concatenate((np.ones(int(signal)), np.zeros(int(background))))\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization scales the input features to have a mean of 0 and a standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\ml_env\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Standardize the input data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\ml_env\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "# Create a vanilla neural network classifier and train it on the training data\n",
    "nn_clf = MLPClassifier(random_state=42)\n",
    "nn_clf.fit(X_train, y_train)\n",
    "\n",
    "# Define the hyperparameter space to search over for the vanilla neural network classifier\n",
    "nn_param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (100, 50), (100, 50, 25)],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [200, 300, 400]\n",
    "}\n",
    "\n",
    "# Set up the grid search using 5-fold cross-validation\n",
    "nn_grid_search = GridSearchCV(nn_clf, nn_param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Perform the grid search on the training set\n",
    "nn_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and the best cross-validation accuracy\n",
    "print('Vanilla Neural Network best hyperparameters:', nn_grid_search.best_params_)\n",
    "print('Vanilla Neural Network best cross-validation accuracy:', nn_grid_search.best_score_)\n",
    "\n",
    "# Re-train the classifier using the best hyperparameters on the full training set\n",
    "nn_clf = nn_grid_search.best_estimator_\n",
    "\n",
    "nn_start_time = time.time()\n",
    "nn_clf.fit(X_train, y_train)\n",
    "nn_end_time = time.time()\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "nn_test_accuracy = nn_clf.score(X_test, y_test)\n",
    "print('Time required to train Vanilla Neural Network classifier:', nn_end_time - nn_start_time)\n",
    "print('Vanilla Neural Network test accuracy:', nn_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVM classifier and train it on the training data\n",
    "svm_clf = SVC(random_state=42)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "svm_param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "svm_grid_search = GridSearchCV(svm_clf, svm_param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print('SVM best hyperparameters:', svm_grid_search.best_params_)\n",
    "print('SVM best cross-validation accuracy:', svm_grid_search.best_score_)\n",
    "\n",
    "svm_clf = svm_grid_search.best_estimator_\n",
    "\n",
    "svm_start_time = time.time()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "svm_end_time = time.time()\n",
    "\n",
    "svm_test_accuracy = svm_clf.score(X_test, y_test)\n",
    "print('Time required to train SVM classifier:', svm_end_time - svm_start_time)\n",
    "print('SVM test accuracy:', svm_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier and train it on the training data\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10, 50, 100, 200],\n",
    "    \"max_depth\": [None, 10, 20, 30],\n",
    "    \"min_samples_split\": [2, 4, 8],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None]\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(rf_clf, svm_param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "\n",
    "print('RF best hyperparameters:', rf_grid_search.best_params_)\n",
    "print('RF best cross-validation accuracy:', rf_grid_search.best_score_)\n",
    "\n",
    "rf_clf = rf_grid_search.best_estimator_\n",
    "rf_start_time = time.time()\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_end_time = time.time()\n",
    "\n",
    "rf_test_accuracy = rf_clf.score(X_test, y_test)\n",
    "print('Time required to train RF classifier:', rf_end_time - rf_start_time)\n",
    "print('RF test accuracy:', rf_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the test set\n",
    "nn_predictions = nn_clf.predict(X_test)\n",
    "svm_predictions = svm_clf.predict(X_test)\n",
    "rf_predictions = rf_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define a dictionary that maps model names to their hyperparameters\n",
    "model_hyperparams = {\n",
    "    'Vanilla Neural Network': nn_grid_search.best_params_,\n",
    "    'SVM': svm_grid_search.best_params_,\n",
    "    'Random Forest': rf_grid_search.best_params_\n",
    "}\n",
    "\n",
    "# Compute the confusion matrix for each model\n",
    "nn_confusion_matrix = metrics.confusion_matrix(y_test, nn_predictions)\n",
    "svm_confusion_matrix = metrics.confusion_matrix(y_test, svm_predictions)\n",
    "rf_confusion_matrix = metrics.confusion_matrix(y_test, rf_predictions)\n",
    "\n",
    "# Print the confusion matrix and hyperparameters for each model\n",
    "print('Vanilla Neural Network confusion matrix:')\n",
    "print(nn_confusion_matrix)\n",
    "print('Vanilla Neural Network hyperparameters:', model_hyperparams['Vanilla Neural Network'])\n",
    "\n",
    "print('SVM confusion matrix:')\n",
    "print(svm_confusion_matrix)\n",
    "print('SVM hyperparameters:', model_hyperparams['SVM'])\n",
    "\n",
    "print('Random Forest confusion matrix:')\n",
    "print(rf_confusion_matrix)\n",
    "print('Random Forest hyperparameters:', model_hyperparams['Random Forest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of each classifier\n",
    "nn_accuracy = metrics.accuracy_score(y_test, nn_predictions)\n",
    "svm_accuracy = metrics.accuracy_score(y_test, svm_predictions)\n",
    "rf_accuracy = metrics.accuracy_score(y_test, rf_predictions)\n",
    "print(\"Vanilla Neural Network Accuracy:\", nn_accuracy)\n",
    "print(\"SVM Accuracy:\", svm_accuracy)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80efd73a90444072c23059b27a682a4b0343647f1b8849b7f8cb3b27a325c7e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
